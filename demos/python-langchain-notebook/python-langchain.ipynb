{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Create a `.env` file containing:\n",
    "```\n",
    "OPENAI_API_KEY=\"<your key here>\"\n",
    "```\n",
    "\n",
    "Install langchain and dewy-client as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install dewy-langchain langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example LangChain without RAG\n",
    "This example shows a simple LangChain application which attempts to answer questions without retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# MODEL=\"gpt-4-0125-preview\"\n",
    "MODEL=\"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(temperature=0.9, model_name=MODEL)\n",
    "\n",
    "llm.invoke(\"What is RAG useful for?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example LangChain with RAG (using Dewy)\n",
    "This example shows what the previous chain looks like using Dewy to retrieve relevant chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving documents in a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from dewy_langchain import DewyRetriever\n",
    "\n",
    "retriever = DewyRetriever.for_collection(\"main\", base_url=\"http://localhost:8000\")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You're a helpful AI assistant. Given a user question and some retrieved content, answer the user question.\n",
    "            If none of the articles answer the question, just say you don't know.\n",
    "\n",
    "            Here is the retrieved content:\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def format_chunks(chunks):\n",
    "    return \"\\n\\n\".join([d.page_content for d in chunks])\n",
    "\n",
    "chain = (\n",
    "    { \"context\": retriever | format_chunks, \"question\": RunnablePassthrough() }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"What is RAG useful for?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain with Citations\n",
    "Based on https://python.langchain.com/docs/use_cases/question_answering/citations#cite-documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    ")\n",
    "\n",
    "class cited_answer(BaseModel):\n",
    "    \"\"\"Answer the user question based only on the given sources, and cite the sources used.\"\"\"\n",
    "\n",
    "    answer: str = Field(\n",
    "        ...,\n",
    "        description=\"The answer to the user question, which is based only on the given sources.\",\n",
    "    )\n",
    "    citations: List[int] = Field(\n",
    "        ...,\n",
    "        description=\"The integer IDs of the SPECIFIC sources which justify the answer.\",\n",
    "    )\n",
    "\n",
    "def format_docs_with_id(docs: List[Document]) -> str:\n",
    "    formatted = [\n",
    "        f\"Source ID: {doc.metadata['chunk_id']}\\nArticle Snippet: {doc.page_content}\"\n",
    "        for doc in docs\n",
    "    ]\n",
    "    return \"\\n\\n\" + \"\\n\\n\".join(formatted)\n",
    "\n",
    "format = itemgetter(\"docs\") | RunnableLambda(format_docs_with_id)\n",
    "\n",
    "# Setup a \"cited_answer\" tool.\n",
    "from langchain.output_parsers.openai_tools import JsonOutputKeyToolsParser\n",
    "output_parser = JsonOutputKeyToolsParser(key_name=\"cited_answer\", return_single=True)\n",
    "\n",
    "llm_with_tool = llm.bind_tools(\n",
    "    [cited_answer],\n",
    "    tool_choice=\"cited_answer\",\n",
    ")\n",
    "answer = prompt | llm_with_tool | output_parser\n",
    "\n",
    "citation_chain = (\n",
    "    RunnableParallel(docs = retriever, question=RunnablePassthrough())\n",
    "    .assign(context=format)\n",
    "    .assign(cited_answer=answer)\n",
    "    # Can't include `docs` because they're not JSON serializable.\n",
    "    .pick([\"cited_answer\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_chain.invoke(\"What is RAG useful for?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Adding documents to the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dewy_client import Client\n",
    "from dewy_client.api.kb import add_document\n",
    "from dewy_client.models import AddDocumentRequest\n",
    "client = Client(base_url=\"http://localhost:8000\")\n",
    "add_document.sync(client=client, body=AddDocumentRequest(\n",
    "    url = \"https://arxiv.org/pdf/2305.14283.pdf\",\n",
    "    collection_id=collection_id,\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledge-7QbvxqGg-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
